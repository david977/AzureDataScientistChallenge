{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning\n",
        "\n",
        "A Convolutional Neural Network (CNN) for image classification is made up of multiple layers that extract features, such as edges, corners, etc; and then use a final fully-connected layer to classify objects based on these features. You can visualize this like this:\n",
        "\n",
        "<table>\n",
        "    <tr><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Fully Connected Layer</td><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td></tr>\n",
        "    <tr><td colspan=4 style='border: 1px solid black; text-align:center;'>Feature Extraction</td><td style='border: 1px solid black; text-align:center;'>Classification</td></tr>\n",
        "</table>\n",
        "\n",
        "*Transfer Learning* is a technique where you can take an existing trained model and re-use its feature extraction layers, replacing its final classification layer with a fully-connected layer trained on your own custom images. With this technique, your model benefits from the feature extraction training that was performed on the base model (which may have been based on a larger training dataset than you have access to) to build a classification model for your own specific set of object classes.\n",
        "\n",
        "How does this help? Well, think of it this way. Suppose you take a professional tennis player and a complete beginner, and try to teach them both how to play raquetball. It's reasonable to assume that the professional tennis player will be easier to train, because many of the underlying skills involved in raquetball are already learned. Similarly, a pre-trained CNN model may be easier to train to classify specific set of objects because it's already learned how to identify the features of common objects, such as edges and corners. Fundamentally, a pre-trained model can be a great way to produce an effective classifier even when you have limited data with which to train it.\n",
        "\n",
        "In this notebook, we'll see how to implement transfer learning for a classification model using PyTorch.\n",
        "\n",
        "## Install and import libraries\n",
        "\n",
        "First, let's install and import the PyTorch libraries we're going to use."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\r\n",
            "Requirement already satisfied: torch==1.7.1+cpu in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (1.7.1+cpu)\r\n",
            "Requirement already satisfied: torchvision==0.8.2+cpu in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (0.8.2+cpu)\r\n",
            "Requirement already satisfied: torchaudio==0.7.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (0.7.2)\r\n",
            "Requirement already satisfied: typing-extensions in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from torch==1.7.1+cpu) (3.7.4.3)\r\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from torch==1.7.1+cpu) (0.8)\r\n",
            "Requirement already satisfied: numpy in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from torch==1.7.1+cpu) (1.19.5)\r\n",
            "Requirement already satisfied: pillow>=4.1.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from torchvision==0.8.2+cpu) (8.2.0)\r\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Other libraries we'll use\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported - ready to use PyTorch 1.7.1+cpu\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "scrolled": false,
        "tags": [],
        "gather": {
          "logged": 1623610562966
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the base model\n",
        "\n",
        "To use transfer learning, we need a base model from which we can use the trained feature extraction layers. The ***resnet*** model is an CNN-based image classifier that has been pre-trained using a huge dataset containing a large number of images of 1000 classes of object, so let's download it and take a look at its layers."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model (download if not already present)\n",
        "model = torchvision.models.resnet34(pretrained=True)\n",
        "\n",
        "print(model)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /home/azureuser/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0.00/83.3M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80b25e620b714d32bebe6848ce91f700"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1623610569532
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the image data\n",
        "\n",
        "The pretrained model has many layers, starting with a convolutional layer that starts the feature extraction process from image data, and ending with a fully-connected linear layer that maps the extracted features to 1000 class labels.\n",
        "\n",
        "For feature extraction to work with our own images, we  need to ensure that the image data we use to train our prediction layer has the same number of features (pixel values) as the images originally used to train the feaure extraction layers. The model does not explicitly give this size, but the first convolutional layer applies by a 7x7 kernel with a stride of 2x2 and results in 64 feature values, so the original size must be 64 x (7 &div; 2), which is 224.\n",
        "\n",
        "PyTorch includes functions for loading and transforming data. We'll use these to create an iterative loader for training data, and a second iterative loader for test data (which we'll use to validate the trained model). The loaders will transform the image data to match the format used to train the original resnet CNN model, convert the image data into *tensors* (which are the core data structure used in PyTorch), and normalize them.\n",
        "\n",
        "Run the following cell to define the data loaders and list the classes for our images."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to ingest data using training and test loaders\n",
        "def load_dataset(data_path):\n",
        "    \n",
        "    # Resize to 256 x 256, then center-crop to 224x224 (to match the resnet image size)\n",
        "    transformation = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Load all of the images, transforming them\n",
        "    full_dataset = torchvision.datasets.ImageFolder(\n",
        "        root=data_path,\n",
        "        transform=transformation\n",
        "    )\n",
        "    \n",
        "    # Split into training (70%) and testing (30%) datasets)\n",
        "    train_size = int(0.7 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "    \n",
        "    # define a loader for the training data we can iterate through in 30-image batches\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=30,\n",
        "        num_workers=0,\n",
        "        shuffle=False\n",
        "    )\n",
        "    \n",
        "    # define a loader for the testing data we can iterate through in 30-image batches\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=30,\n",
        "        num_workers=0,\n",
        "        shuffle=False\n",
        "    )\n",
        "        \n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "# Now load the images from the shapes folder\n",
        "import os  \n",
        "data_path = 'data/shapes/'\n",
        "\n",
        "# Get the iterative dataloaders for test and training data\n",
        "train_loader, test_loader = load_dataset(data_path)\n",
        "\n",
        "# Get the class names\n",
        "classes = os.listdir(data_path)\n",
        "classes.sort()\n",
        "print('class names:', classes)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class names: ['circle', 'square', 'triangle']\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1623610578497
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a prediction layer\n",
        "\n",
        "We downloaded the complete *resnet* model including its final **fc** linear layer. This fully-connected linear layer takes 512 inputs (the extracted features) and produces 1000 outputs (class predictions based on the original training image classes). We need to replace this layer with one that takes the same number of inputs (so we can use the same number of extracted features), but produces a prediction for each of our image classes.\n",
        "\n",
        "We also need to freeze the feature extraction layers to retain the trained weights. Then when we train the model using our images, only the final prediction layer will learn new weight and bias values - the pre-trained weights already learned for feature extraction will remain the same."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the existing feature extraction layers to read-only\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the prediction layer\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(classes))\n",
        "\n",
        "# Now print the full model, which will include the feature extraction layers of the base model and our prediction layer\n",
        "print(model)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1623610586072
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "With the layers of the CNN defined, we're ready to train it using our image data. The weights used in the feature extraction layers from the base resnet model will not be changed by training, only the final linear layer that maps the features to our shape classes will be trained."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    print(\"Epoch:\", epoch)\n",
        "    # Process the images in batches\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Use the CPU or GPU as appropriate\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # Reset the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Push the data forward through the model layers\n",
        "        output = model(data)\n",
        "        \n",
        "        # Get the loss\n",
        "        loss = loss_criteria(output, target)\n",
        "        \n",
        "        # Keep a running total\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Print metrics for every 10 batches so we see some progress\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Training set [{}/{} ({:.0f}%)] Loss: {:.6f}'.format(\n",
        "                batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            \n",
        "    # return average loss for the epoch\n",
        "    avg_loss = train_loss / (batch_idx+1)\n",
        "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
        "    return avg_loss\n",
        "            \n",
        "            \n",
        "def test(model, device, test_loader):\n",
        "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        batch_count = 0\n",
        "        for data, target in test_loader:\n",
        "            batch_count += 1\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            # Get the predicted classes for this batch\n",
        "            output = model(data)\n",
        "            \n",
        "            # Calculate the loss for this batch\n",
        "            test_loss += loss_criteria(output, target).item()\n",
        "            \n",
        "            # Calculate the accuracy for this batch\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            correct += torch.sum(target==predicted).item()\n",
        "\n",
        "    # Calculate the average loss and total accuracy for this epoch\n",
        "    avg_loss = test_loss/batch_count\n",
        "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        avg_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    # return average loss for the epoch\n",
        "    return avg_loss\n",
        "    \n",
        "    \n",
        "# Now use the train and test functions to train and test the model    \n",
        "\n",
        "device = \"cpu\"\n",
        "if (torch.cuda.is_available()):\n",
        "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
        "    device = \"cuda\"\n",
        "print('Training on', device)\n",
        "\n",
        "# Create an instance of the model class and allocate it to the device\n",
        "model = model.to(device)\n",
        "\n",
        "# Use an \"Adam\" optimizer to adjust weights\n",
        "# (see https://pytorch.org/docs/stable/optim.html#algorithms for details of supported algorithms)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Specify the loss criteria\n",
        "loss_criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "# Track metrics in these arrays\n",
        "epoch_nums = []\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "\n",
        "# Train over 3 epochs (in a real scenario, you'd likely use many more)\n",
        "epochs = 3\n",
        "for epoch in range(1, epochs + 1):\n",
        "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "        test_loss = test(model, device, test_loader)\n",
        "        epoch_nums.append(epoch)\n",
        "        training_loss.append(train_loss)\n",
        "        validation_loss.append(test_loss)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cpu\n",
            "Epoch: 1\n",
            "Training set [0/840 (0%)] Loss: 1.057548\n",
            "Training set [300/840 (36%)] Loss: 0.618081\n",
            "Training set [600/840 (71%)] Loss: 0.291469\n",
            "Training set: Average loss: 0.556850\n",
            "Validation set: Average loss: 0.317216, Accuracy: 355/360 (99%)\n",
            "\n",
            "Epoch: 2\n",
            "Training set [0/840 (0%)] Loss: 0.301849\n",
            "Training set [300/840 (36%)] Loss: 0.226347\n",
            "Training set [600/840 (71%)] Loss: 0.124419\n",
            "Training set: Average loss: 0.192180\n",
            "Validation set: Average loss: 0.128376, Accuracy: 358/360 (99%)\n",
            "\n",
            "Epoch: 3\n",
            "Training set [0/840 (0%)] Loss: 0.147011\n",
            "Training set [300/840 (36%)] Loss: 0.194711\n",
            "Training set [600/840 (71%)] Loss: 0.084740\n",
            "Training set: Average loss: 0.121245\n",
            "Validation set: Average loss: 0.082887, Accuracy: 358/360 (99%)\n",
            "\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "scrolled": false,
        "tags": [],
        "gather": {
          "logged": 1623610880044
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View the loss history\n",
        "\n",
        "We tracked average training and validation loss for each epoch. We can plot these to verify that the loss reduced over the training process and to detect *over-fitting* (which is indicated by a continued drop in training loss after validation loss has levelled out or started to increase)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(epoch_nums, training_loss)\n",
        "plt.plot(epoch_nums, validation_loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['training', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1623610880301
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate model performance\n",
        "\n",
        "We can see the final accuracy based on the test data, but typically we'll want to explore performance metrics in a little more depth. Let's plot a confusion matrix to see how well the model is predicting each class."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Pytorch doesn't have a built-in confusion matrix metric, so we'll use SciKit-Learn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Set the model to evaluate mode\n",
        "model.eval()\n",
        "\n",
        "# Get predictions for the test data and convert to numpy arrays for use with SciKit-Learn\n",
        "print(\"Getting predictions from test set...\")\n",
        "truelabels = []\n",
        "predictions = []\n",
        "for data, target in test_loader:\n",
        "    for label in target.cpu().data.numpy():\n",
        "        truelabels.append(label)\n",
        "    for prediction in model.cpu()(data).data.numpy().argmax(1):\n",
        "        predictions.append(prediction) \n",
        "\n",
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(truelabels, predictions)\n",
        "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes, rotation=45)\n",
        "plt.yticks(tick_marks, classes)\n",
        "plt.xlabel(\"Predicted Shape\")\n",
        "plt.ylabel(\"Actual Shape\")\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions from test set...\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the trained model\n",
        "\n",
        "Now that we've trained the model, we can use it to predict the class of an image."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a random image (of a square, circle, or triangle)\n",
        "def create_image (size, shape):\n",
        "    from random import randint\n",
        "    import numpy as np\n",
        "    from PIL import Image, ImageDraw\n",
        "    \n",
        "    xy1 = randint(10,40)\n",
        "    xy2 = randint(60,100)\n",
        "    col = (randint(0,200), randint(0,200), randint(0,200))\n",
        "\n",
        "    img = Image.new(\"RGB\", size, (255, 255, 255))\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    \n",
        "    if shape == 'circle':\n",
        "        draw.ellipse([(xy1,xy1), (xy2,xy2)], fill=col)\n",
        "    elif shape == 'triangle':\n",
        "        draw.polygon([(xy1,xy1), (xy2,xy2), (xy2,xy1)], fill=col)\n",
        "    else: # square\n",
        "        draw.rectangle([(xy1,xy1), (xy2,xy2)], fill=col)\n",
        "    del draw\n",
        "    \n",
        "    return img\n",
        "    \n",
        "# Function to predict the class of an image\n",
        "def predict_image(classifier, image):\n",
        "    import numpy\n",
        "    \n",
        "    # Set the classifer model to evaluation mode\n",
        "    classifier.eval()\n",
        "    \n",
        "    # Apply the same transformations as we did for the training images\n",
        "    transformation = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Preprocess the image\n",
        "    image_tensor = transformation(image).float()\n",
        "\n",
        "    # Add an extra batch dimension since pytorch treats all inputs as batches\n",
        "    image_tensor = image_tensor.unsqueeze_(0)\n",
        "\n",
        "    # Turn the input into a Variable\n",
        "    input_features = Variable(image_tensor)\n",
        "\n",
        "    # Predict the class of the image\n",
        "    output = classifier(input_features)\n",
        "    index = output.data.numpy().argmax()\n",
        "    return index\n",
        "\n",
        "\n",
        "# Now let's try it with a new image\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import os, shutil\n",
        "\n",
        "# Create a random test image\n",
        "shape = classes[randint(0, len(classes)-1)]\n",
        "img = create_image ((128,128), shape)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(img)\n",
        "\n",
        "\n",
        "index = predict_image(model, img)\n",
        "print(classes[index])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learn more\n",
        "\n",
        "* [PyTorch Documentation](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}